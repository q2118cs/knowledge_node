# Kafka介绍

kafka由Linkedin公司开发，是一个分布式、分区、多副本、多生产者、多订阅者，基于zookeeper协调的分布式日志系统，可用于web/nginx日志、访问日志、消息服务等，现在已经成为Apache顶级项目，Kafka的主要设计目标为：

- 以时间复杂度O(1)方式提供消息持久化能力，既使对TB级别以上数据也能保证常数时间访问
- 高吞吐量。即使在廉价机器上也能做到单机支持每秒100K的消息传输
- 支持Kafka Server间的分区及分布式消费，同时保证每个partition内的消息顺序传输
- 支持离线数据处理及实时数据处理
- 支持在线水平扩展

# Kafka基本架构

**消息和批次**

kafka的数据单元称为消息，消息由字节数组组成。为了提高效率，消息会被分批写入kafka，批次就是一组消息，这些消息属于同一个主题和分区，分成批次后可以减少网络开销，提高并发量，但会造成单条消息的发送时间变长。

**主题和分区**

kafka的消息通过主题分类，就想是数据库中的表，主题可以被分为若干个分区，每个分区的消息都是有序的。

**生产者和消费者**

一个消息被发送到特定的主题上，生产者默认情况下把消息均衡分布在主题的所有分区上。消费者通过偏移量区分已读过的消息从而消费消息。消费者是消费组的一部分，消费组保证每个分区只能被一个消费者使用，避免重复消费。

**broker和集群**

一个独立的kafka服务器被称为broker。broker接收来自生产者的消息并为消息设置偏移量，提交消息到磁盘保存。单个broker可以轻松处理数千个分区以及每秒百万级的消息量。

每个集群中都有一个broker来做集群控制器，控制器负责将分区分配给broker并且监控broker，该broker被称为分区首领。一个分区可以分配给多个broker，此时会发生分区复制，分区复制提供了消息冗余、高可用，副本分区不负责消息的读写。

# Kafka核心概念

## producer

生产者生产并发送消息，一般情况下消息会被发送到一个特定的topic中。

- 默认情况通过轮训将消息均衡的分布在主题的所有分区上
- 生产者也可以将消息写到指定的分区上，通过消息键和分区器来实现，分区器为消息生成一个hash值并映射到指定分区上，这样保证同一个键的消息会被写到同一个分区上。
- 生产者也可以使用自定义的分区器，根据不同业务规则映射到指定分区。

## Consumer

消费者读取并消费消息。消费者订阅一个或多个主题，按照消息的生成顺序读取他们。

消费者通过检查消息的偏移量来区分已经读取过的消息。偏移量是一种元数据，它是一个不断递增的整数值，创建消息的时候，kafka会把它添加到消息里。在给定的分区里，消息的偏移量是唯一的。消费者将每个分区最后读取的消息偏移量保存在kafak或者zookeeper上，如果消费者关闭不会影响到读取状态。

消费者是消费组的一部分，群组保证每个分区只能被一个消费者使用。分区和消费者是一对多的关系。

如果一个消费者失效，消费组里的其他消费者可以接管失效消费者的工作。

## Broker

一个独立的kafka服务器被称为broker，broker为消费者提供服务。

- 如果主题有N个分区，集群有N个broker，那么每个broker存储该主题的一个分区；
- 如果主题有N个分区，集群有N+M个broker，那么有N个broker存储该主题的一个分区，剩下的M个broker不存储分区数据
- 如果主题有N个分区，集群的broker少于N个，那么一个broker存储一个或多个分区。实际环境中要避免这种情况。

## Topic

每条发送到kafka集群的消息都有一个类别被称为Topic，物理上不同的topic分开存储，类比于数据库中的表

## Partition

主题可以被分为若干个分区，一个分区就是一个提交日志，消息以追加的方式写入分区，以FIFO方式顺序读取

无法在整个主题范围内保证消息的顺序，可以保证单个分区的顺序

kafka通过分区来实现数据容易和伸缩性

如果需要严格保证消息的消费顺序，需要将partition设置为1

## Replicas

每个分区有多个副本，副本保存在broker上，每个broker可以保存成百上千个属于不同主题和分区的副本

副本有两种类型：

1. 首领副本：每个分区都有一个首领副本，为了保证一致性，所有请求都会经过此副本
2. 跟随者副本：首领以外的副本，不处理来自客户端的请求，唯一任务是从首领复制消息，保持与首领的一致性。如果首领发生崩溃，其中的一个跟随者会提升为首领。跟随者副本分为同步副本和不同步副本，只有同步副本可以切换成首领副本。

分区中所有的副本统称为AR(Assigned Replicas)，AR包含ISR(In-Sync Replicas)和OSR(Out-Sync Replication)

1. ISR：与首领副本保持一定程度同步的副本组成ISR。在与首领副本同步的时候会有一定程度滞后，这个范围可以通过参数控制。
2. OSR：与首领副本同步滞后过多的副本组成OSR。正常情况下所有副本都应与首领副本保持一致，即AR=ISR，OSR为空。

## Offset

1. 生产者偏移量：每个分区都有一个偏移量，这个偏移量就是生产者的偏移量，同时也是分区最新最大的offset
2. 消费者偏移量：主要为了保证消息不会重复消费，比如生产者便宜量是12，消费者1偏移量是9，那么代表0-8已经消费过了，等下次再消费的时候可以选择接着上次的消费，也可以选择从头消费，或者跳到最新的记录从“现在“开始消费。

HW：High Watermark，表示一个特定消息的偏移量，这个偏移量之前的数据肯定已经同步完了，消费者只能拉取到这个offset之前的消息。

LEO：Log End Offset，表示当前日志文件中下一个待写入消息的offset。

# Kafka高级特性

## 生产者

![image-20200830203756087](/Users/sunderui/codes/git/knowledge_node/docs/message_queue/kafka_producer.png)

Producer创建的时候会创建Sender线程并设置为守护线程，生产的消息先经过拦截器-序列化器-分区器，然后将消息缓冲在缓冲区，整个过程是个异步过程；当缓冲区数据大小达到batch.size或者linger.ms达到上限，批次就会发送；批次发送后发到指定分区，然后落盘到broker；如果生产者配置了重试策略当失败的时候会进行重试；落盘成功后会将生产元数据返回给生产者，元数据可通过阻塞或者回调返回。

常用参数：

| 属性             | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| acks             | 控制发送消息的持久性！<br> 0：生产者不等broker返回任何信息，只要将消息放到缓冲区就认为消息发送成功，不能保证服务器已经收到该消息，重试设置也不会生效，客户端收到的偏移量永远是-1 ;<br>1：**默认值**，leader副本将记录存到本地日志就响应客户端，不等待follower副本确认，可能会丢失消息；<br>all：等同于-1，leader等待所有副本确认该消息，保证了消息的持久性。 |
| compression.type | 生成数据的压缩格式！默认none，不压缩。可选值：none、gzip、snappy、lz4 |

